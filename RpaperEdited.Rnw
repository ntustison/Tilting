%% This is file `elsarticle-template-1-num.tex',
%%
%% Copyright 2009 Elsevier Ltd
%%
%% This file is part of the 'Elsarticle Bundle'.
%% ---------------------------------------------
%%
%% It may be distributed under the conditions of the LaTeX Project Public
%% License, either version 1.2 of this license or (at your option) any
%% later version.  The latest version of this license is in
%%    http://www.latex-project.org/lppl.txt
%% and version 1.2 or later is part of all distributions of LaTeX
%% version 1999/12/01 or later.
%%
%% The list of all files belonging to the 'Elsarticle Bundle' is
%% given in the file `manifest.txt'.
%%
%% Template article for Elsevier's document class `elsarticle'
%% with numbered style bibliographic references
%%
%% $Id: elsarticle-template-1-num.tex 149 2009-10-08 05:01:15Z rishi $
%% $URL: http://lenova.river-valley.com/svn/elsbst/trunk/elsarticle-template-1-num.tex $
%%

\documentclass[final,5p,times,twocolumn]{elsarticle}

%% Use the option review to obtain double line spacing
%% \documentclass[preprint,review,12pt]{elsarticle}

%% Use the options 1p,two column; 3p; 3p,twocolumn; 5p; or 5p,twocolumn
%% for a journal layout:
%% \documentclass[final,1p,times]{elsarticle}
%% \documentclass[final,1p,times,twocolumn]{elsarticle}
%% \documentclass[final,3p,times]{elsarticle}
%% \documentclass[final,3p,times,twocolumn]{elsarticle}
%% \documentclass[final,5p,times]{elsarticle}
%% \documentclass[final,5p,times,twocolumn]{elsarticle}

\usepackage{epigraph}
\usepackage{color}
\usepackage{multirow,booktabs,ctable,array}
\usepackage{lscape}
\usepackage{amsmath}
\usepackage{lineno}
%%\usepackage{ulem}
\usepackage{setspace}
\usepackage{listings}
\usepackage{float}
\usepackage{listings}
\usepackage{color,colortbl}
\usepackage{rccol}
\usepackage[table]{xcolor}

    \definecolor{listcomment}{rgb}{0.0,0.5,0.0}
    \definecolor{listkeyword}{rgb}{0.0,0.0,0.5}
    \definecolor{listnumbers}{gray}{0.65}
    \definecolor{listlightgray}{gray}{0.955}
    \definecolor{listwhite}{gray}{1.0}
    \definecolor{lightcyan}{rgb}{0.88,1,1}

\usepackage[draft]{changes}
\definechangesauthor[name={Hans Johnson}, color=green]{hj}
\definechangesauthor[name={Torsten Rolfing}, color=red]{tr}
\definechangesauthor[name={Nick Tustison}, color=orange]{nt}
\definechangesauthor[name={Brian Avants}, color=purple]{ba}
\definechangesauthor[name={Arno Klein}, color=blue]{ak}
\definechangesauthor[name={Satrajit Ghosh}, color=magenta]{sg}


\floatstyle{plain}
\newfloat{command}{thp}{lop}
\floatname{command}{Command}

%\usepackage[nomarkers,notablist]{endfloat}

%% if you use PostScript figures in your article
%% use the graphics package for simple commands
%% \usepackage{graphics}
%% or use the graphicx package for more complicated commands
%% \usepackage{graphicx}
%% or use the epsfig package if you prefer to use the old commands
%% \usepackage{epsfig}

%% The amssymb package provides various useful mathematical symbols
\usepackage{amssymb}
%% The amsthm package provides extended theorem environments
% \usepackage{amsthm}

 \usepackage{makecell}

%% The lineno packages adds line numbers. Start line numbering with
%% \begin{linenumbers}, end it with \end{linenumbers}. Or switch it on
%% for the whole article with \linenumbers after \end{frontmatter}.
%% \usepackage{lineno}

%% natbib.sty is loaded by default. However, natbib options can be
%% provided with \biboptions{...} command. Following options are
%% valid:

%%   round  -  round parentheses are used (default)
%%   square -  square brackets are used   [option]
%%   curly  -  curly braces are used      {option}
%%   angle  -  angle brackets are used    <option>
%%   semicolon  -  multiple citations separated by semi-colon
%%   colon  - same as semicolon, an earlier confusion
%%   comma  -  separated by comma
%%   numbers-  selects numerical citations
%%   super  -  numerical citations as superscripts
%%   sort   -  sorts multiple citations according to order in ref. list
%%   sort&compress   -  like sort, but also compresses numerical citations
%%   compress - compresses without sorting
%%
%% \biboptions{comma,round}

% \biboptions{}

\providecommand{\OO}[1]{\operatorname{O}\bigl(#1\bigr)}

\graphicspath{
             {./Figures/}
             }

\long\def\symbolfootnote[#1]#2{\begingroup%
\def\thefootnote{\fnsymbol{footnote}}\footnote[#1]{#2}\endgroup}

% \usepackage{Sweave}
\begin{document}
% \SweaveOpts{concordance=TRUE}
% \input{Rpaper-concordance}


\begin{frontmatter}

\title{Instrumentation bias in the use and evaluation of scientific software}

\author[label1]{Nicholas J.~Tustison
  \fnref{label0}}
  \fntext[label0]{\scriptsize Corresponding author:  PO Box 801339, Charlottesville, VA 22908; T:  434-924-7730; email address:  ntustison@virginia.edu.
  }
\author[label3]{Hans Johnson}
\author[label4]{Torsten Rohlfing}
\author[label5]{Arno Klein}
\author[label6]{Satrajit S.~Ghosh}
\author[label7]{Luis Ibanez}
\author[label2]{Brian B.~Avants}
\address[label1]{Department of Radiology and Medical Imaging, University of Virginia, Charlottesville, VA, USA}
\address[label2]{Penn Image Computing and Science Laboratory, University of Pennsylvania,
                Philadelphia, PA, USA}
\address[label3]{Department of Psychiatry, University of Iowa, Iowa City, IA, USA}
\address[label4]{SRI International, Menlo Park, CA, USA}
\address[label5]{Department of Psychiatry and Behavioral Science, State University of New York, Stony Brook, NY, USA}
\address[label6]{McGovern Institute	for	Brain	Research, Massachusetts Institute of Technology, Cambridge, MA, USA}
\address[label7]{Kitware, Inc., Clifton Park, NY, USA}

%\maketitle

\linenumbers

\begin{abstract}
Exploration of neuroscience hypotheses has been  enhanced
by the increased availability of high-performance computational resources
such as XSEDE%
\footnote{
https://www.xsede.org/
}; large-scale developmental efforts such as the Insight Toolkit and the 
R statistical project; and algorithmic advancements for data transformations and 
processing.
An integral component of a rigorous vetting process for
novel algorithmic techniques includes comparison with other methods previously
established within the community.  Public availability, including
open source distribution, of established packages has significantly
facilitated these comparative evaluations, particularly when the packages
provide access to an open source distribution and are indexed in a resource like the 
Neuroimaging Informatics Tools and Resources Clearinghouse%
\footnote{
http://www.nitrc.org
}.  
However, complementing the
recent set of papers pointing to serious methodological and statistical
bias considerations in neuroimaging research, we point out potential
issues associated with measurement bias in comparative algorithmic
evaluations.  In this paper we propose a set of guidelines for authors and 
reviewers to minimize these evaluation biases.
\end{abstract}
\begin{keyword}
open science \sep open data \sep comparison \sep reproducibility  \sep software \sep instrument bias
%% keywords here, in the form: keyword \sep keyword
\end{keyword}

\end{frontmatter}
%
%
\newpage

%% MSC codes here, in the form: \MSC code \sep code
%% or \MSC[2008] code \sep code (2000 is the default)

%%
%% Start line numbering here if you want
%%
% \linenumbers

%% The Appendices part is started with the command \appendix;
%% appendix sections are then done as normal sections
%% \appendix

%% \section{}
%% \label{}

%% References
%%
%% Following citation commands can be used in the body text:
%% Usage of \cite is as follows:
%%   \citep{key}          ==>>  [#]
%%   \cite[chap. 2]{key} ==>>  [#, chap. 2]
%%   \citet{key}         ==>>  Author [#]

%% main text


\section{Introduction}

\epigraph{By honest I don't mean that you only tell what's true. But you make clear the
entire situation. You make clear all the information that is required for somebody else
who is intelligent to make up their mind.}{Richard Feynman}


The neuroscience community has significantly benefitted from the proliferation
of imaging-related analysis software packages.
Established packages such as SPM \cite{Ashburner2012},
the FMRIB Software Library (FSL) \cite{Jenkinson2012}, Slicer \cite{fedorov2012} and
the AFNI toolkit \cite{cox2012} have aided neuroimaging researchers
around the world in performing complex analyses as part of
ongoing neuroscience research.
In conjunction with distributing robust software tools, neuroimaging 
packages also continue to incorporate
algorithmic innovation for improvement
in analysis tools.

As fellow scientists who actively participate in neuroscience
research through our many contributions to the Insight
Toolkit
(e.g. \cite{johnson2007,tustison2009a,tustison2012a})
and other packages such as the Advanced Normalization Tools (ANTs),%
\footnote{
http://www.picsl.upenn.edu/ANTs
}
we have noticed an increasing number of publications in which a principal
component involves the comparison of algorithms which, in principle, is
a good thing.  We are concerned, however, by the lack of detail with 
which these comparisons are
often presented and the corresponding possibility of {\it instrumentation
bias} \cite{sackett1979} (considering software  as a type of
instrument requiring proper ``calibration'' for accurate measurements).
Based on our experience (including mistakes that we ourselves have made),
we propose a preliminary set of guidelines for minimal
reporting of algorithmic usage to minimize such bias, understanding that
the discussion will likely
elicit a much more comprehensive response from the larger neuroscience community.

Previous articles have discussed similar concerns, oftentimes
within a specific application context (e.g. fMRI reporting \cite{poldrack2008}),
we are more generally interested in comparative evaluations of software
and provide additional discussion through experience and actual
examples found within the literature.
It is hoped that this commentary serves to raise awareness of both
authors and reviewers to such problematic issues (not unlike other
recent articles detailing additional potential sources of methodological
bias \cite{kriegeskorte2009,vul2012,tustison2012}).

\section{Guidelines}

A comparative analysis paper's longevity and impact on future scientific
explorations is directly related to the completeness of the evaluation.  
A complete evaluation requires preparatory effort (before any
experiment is performed) and effort to publish its details and results.
Here, we suggest general guidelines for both of these steps most of which
derive from basic scientific principles of clarity and reproducibility.

\subsection{Designing the evaluation study}
\subsubsection{Rigorously define the algorithm's scope}
The only algorithm that is capable of outperforming all others on all
problems is exhaustive search, assuming a perfect objective function.
As this is not feasible in practice, many algorithms employ
prior knowledge such as regularization or other heuristics that
restrict the search space to the relevant parameters for a narrowly 
specified problem.  The very idea that one (e.g., registration) algorithm 
could perform better than all other algorithms on
all types of data is fundamentally flawed.  Indeed, the ``No Free Lunch
Theorems'' provide bounds on solution quality that show otherwise
i.e. specifically that ``improvement of performance in problem-solving hinges on using prior information to match procedures to problems''\cite{Wolpert1997}.

\subsubsection{Demand that the algorithm developers provide default parameters for the 
context being investigated}
Expert knowledge of a specific program and/or algorithm is most
likely found with the original developers who would be in a position to
provide optimal parameterization. In comparing FAST (FSL), 
FreeSurfer, and SPM5, the authors of the study \cite{klauschen2009}
contacted the software authors ``to verify that the approach use[d] in the
publication is recommended.''  Similarly, for a comparison of brain extraction
methods, the authors of the study \cite{iglesias2011} contacted the
authors of each of the six competitor algorithms who were ``invited to provide 
parameter values for each of the three [public] datasets.''  The study authors itemized 
the responses from each of the contacts and even accommodated a pre-processing
request from one of the algorithm's authors.


Developers should actively facilitate
comparisons by providing default parameter files, sample scripts,
data or possible command line calls.  For example, the developers of
the image registration program elastix \cite{klein2010} provide an
assortment of parameter files on the wiki page%
\footnote{
  http://elastix.bigr.nl/wiki/index.php/Parameter\_file\_database
}
listed in tabular format complete with  short
description (including applied modality and object of interest) and
any publications which used that specific parameter file.  Additionally,
other packages are associated with discussion forums where users can
query regarding usage (even though in archived form such unstructured 
information sources can be difficult to query and navigate).

\subsubsection{Do not implement your own version of the algorithm particularly
if one is available from the original authors.
If you must re-implement, consider making your implementation available}

Collective experience in developing software has taught us the
difficulty of translating descriptions found in
articles into functional code and that much is left unstated
in presenting core algorithmic ideas.  Ideally, the authors,
in conjunction with the manuscript, should provide an instantiation
of the code.  Unfortunately, as observed in \cite{kovacevic2006}
this is often not the case (even in terms of pseudocode).
As a result, comparative evaluations are sometimes carried out using
code developed not by the original authors but rather the
group doing the comparison.  This is not ideal as it is questionable
whether or not the authors have sufficient motivation to ensure that
their instantiation of the algorithm is working correctly. For
example, in \cite{clarkson2011}, the authors compared three algorithms
for estimating cortical thickness.  Two of the algorithms were
coded by the authors while the third was used ``off the shelf'' 
having been extensively
tuned and well-documented by the original authors.
Thus, a natural question to ask is whether the performance difference
is due to the algorithm itself or the parameter tuning.

\subsubsection{Co-authors should verify findings}

Although different journals have varying guidelines for determining
co-authorship, there is at least an implied sense of responsibility for
an article's contents assumed by each of the co-authors.  Strategies
taken by journal editorial boards are used to reduce undeserving
authorship attribution such as requiring the listing of the specific
contributions of each co-author.  Additional proposals have included
signed statements of responsibility for the contents of the article
\cite{nature2007}.  We suggest that at least one co-author
independently verify a subset of the results by running the processing
on their own computer.   The point of this exercise is to verify not only
reproducibility but also that the process can be explained in
sufficient detail.

\subsubsection{Comparisons should be performed on publicly available data}
For reasons of reproducibility and transparency, evaluations should be performed
using publicly available data sets.  Given the rather large number
of such institutional efforts including
IXI,%
\footnote{
http://www.brain-development.org
}
NKI,%
\footnote{
http://fcon\_1000.projects.nitrc.org/indi/pro/nki.html
}
OASIS,%
\footnote{
http://www.oasis-brains.org
}
Kirby,%
\footnote{
http://mri.kennedykrieger.org/databases.html
},
LONI,%
\footnote{
http://www.loni.ucla.edu/Research/Databases/
}, and
others, evaluations should include (if not be exhausted by)
comparisons using such data.  While evaluation on specific abnormal cohorts
might exclude such possibilities, such specific evaluations should
be extensively motivated in the introduction and/or discussion.
For example, if a particular algorithm with general application is
found to perform better
on a private cohort of Parkinson's disease subject data, reasons
for performance disparity should be offered and supplemented with
analysis on public data.


\subsection{Publishing the evaluation}

\subsubsection{Include parameters}
A best practice exemplar of comparative evaluation studies
is the work by Klein et al. \cite{Klein2009} where
14 nonlinear registration algorithms were compared using
four publicly available, labeled brain MRI data sets.
As part of the study, the authors themselves tuned
the parameters for their algorithm to ensure good
performance which were then distributed 
on Prof. Klein's website.%
\footnote{
http://mindboggle.info/papers/evaluation\_NeuroImage2009.php
}
This practice promotes reproducibility, fairness, and transparency in
the evaluation and is perhaps most fundamental in these types
of studies for establishing conclusions regarding the relative
performance of algorithms.

In contrast, not specifying parameters
leaves one susceptible to criticisms of confirmation and/or
instrumentation bias \cite{sackett1979}.  For example, in
a recent paper \cite{haegelen2013} (similar issues can
be found in \cite{luo2013,wu2013}), the authors compared
their ANIMAL registration algorithm with SyN \cite{avants2011} and determined
that ``[r]egistration with ANIMAL was better than with SyN for
the left thalamus'' in a cohort of Parkinson's disease patients.
One of the stated reasons for using ANIMAL was ``because of
[the authors']
extensive experience with this technique.''
  However, for SyN,
the authors provide no equivalent assurance---no contact
was made with the primary developers of ANTs (SyN) nor
were any parameters listed for independent verification
by reviewers or readers of the article.
Given the authors' relationship with one of the algorithms
and likely predisposition towards its relative performance,
particular caution has to be provided against possible confirmation
bias.  


\subsubsection{Provide details as to the source of the algorithm}
The precise origin
should be provided for any code or package used during the
evaluation.  For example, N4 \cite{tustison2010} is a well-known
bias correction algorithm first made available as a tech report
in the Insight Journal \cite{tustison2009a} for subsequent introduction
into the Insight Toolkit.  However, since
its inclusion, different instantiations have been made available.
Apart from the source code that accompanied the original tech
report, N4 is also available in ANTs (the only version directly maintained
by the original authors).
It is also available as a module in Slicer,%
\footnote{
http://www.slicer.org/slicerWiki/index.php/Documentation/4.2/Modules/N4ITKBiasFieldCorrection
}
a wrapper of the Slicer module in nipype,%
\footnote{
http://www.mit.edu/~satra/nipype-nightly/interfaces/generated/nipype.interfaces.slicer.filtering.n4itkbiasfieldcorrection.html
}
a module in \verb#c3d#,%
\footnote{
http://www.itksnap.org8/pmwiki/pmwiki.php?n=Convert3D.Documentation
}
and as a plugin in the BRAINS suite.%
\footnote{
http://www.nitrc.org/plugins/mwiki/index.php/brains:N4ITK
}
While each version is dependent on the original source
code, there could exist subtle variations which can affect performance.
For example, the parameters governing the spline distance and
number of iterations directly affect outcome and should be tuned
based on application.  However, as a particular case, the \verb#c3d#
implementation hard-codes
these values with no access to modify them by the user, potentially
leading to suboptimal performance.

\subsubsection{Provide computational platform details of the evaluation}
A recent article \cite{gronenschild2012} pointed out significant
differences in FreeSurfer output that varied with  release
version and with operating systems.  While the former is to be
expected given upgrades and bug fixes which occur between releases,
the latter underscores both the need for consistency in study processing
as well as the reporting of computational details for reproducibility.

\subsubsection{Pre- and post-processing steps need to be supplied}
In addition to disclosure of all parameters associated with the
methods to be compared, all processing steps from the raw
to the final processed images in the workflow
need to be specified.    Justification for any deviation
of steps between algorithms needs to be provided.  Additionally,
failures during processing should be mentioned and whether
or not these data were excluded from subsequent analysis.


\subsubsection{Resulting images should be publicly posted 
and/or online public resources should be used
}

The current publishing paradigm limits the quantity of
results that can be posted.  There are only so many pages
allowed for a particular publication and displaying every
slice of every processed image, for example, is not feasible.
This results in possible selection bias where results provided
in the manuscript are selected by the authors for demonstrating
the effect postulated at the onset of the study.  Thus, differences
in performance assessment tend to be exaggerated based strictly
on visual representations in the paper.  Resources for data
sharing include MIDAS,%
\footnote{
http://www.midasplatform.org
}
XNAT,%
\footnote{
http://xnat.org
}
and slicedrop.%
\footnote{
http://slicedrop.com
}

Alternatively (or additionally), online resources can
be used to publicly post and compare results.  Shattuck et al. 
\cite{shattuck2009} describe the LONI Segmentation Validation Engine%
\footnote{
http://sve.loni.ucla.edu
}
which can be used to evaluate individual algorithms (and parameter set 
variants) for brain segmentation on publicly available
data sets and compare with previously posted results.  A top ranking
outcome provides significant external validation for publishing newly
proposed methodologies (e.g. \cite{eskildsen2012}).

%figshare

\subsubsection{Properly contextualize comparisons and motivate performance differences}
In addition to algorithmic and study specifics, it is important to
discuss potential limitations concerning qualitative and/or quantitative
assessment metrics.  In \cite{rohlfing2012}, T. Rohlfing pointed out
deficiencies in using standard overlap measures and image similarity
metrics in quantifying performance.  Other issues, such as biological
plausibility of the resulting transforms, need to also be considered.

Also important for inclusion is discussion on the possible reasons for performance
disparity.  If one algorithm outperforms another, reporting of those findings
would be much more significant if the authors discussed possible reasons
for relative  performance levels.  This simply constitutes evidence for
the claims of the paper.

\subsection{Pertinency of the current publishing paradigm}
There has been much discussion amongst the broad scientific
community regarding current publishing practices.  Issues 
such as open access to scientific findings and exorbitant 
subscription costs have caused many scientists to rethink
their approaches to reporting research.  In addition to
such issues,
we point out that these traditional venues are often 
suboptimal for publishing evaluations given the dynamic nature
of the software development process.    In
addition to considering public availability, we would also encourage
authors to consider more dynamic venues for posting relevant material
and updates.  Online utilities such as sourceforge and github not only facilitate
collaboration and code distribution but often support wikis
and discussion forums which provide an auxiliary resource to
original publication(s).

\section{Conclusion}
Considering the permeation of the computational sciences in doing
nueroimaging research, certain safeguards should be in place to prevent
(or at least minimize) potential biases and errors that can
unknowingly effect study outcomes.  There is no vetting agency for
ensuring that analysis programs used for research are reasonably error-free.
In addition, these software packages are simply ``black
boxes'' to many researchers who are not formally trained to debug
code.  And even to those of us who are, debugging other people's code
is oftentimes very difficult.  To this end, algorithmic comparisons are
a very good way of evaluating general performance.  It is hoped that the
proposed set of guidelines set forth in this editorial aid the community
for future comparative assessments.


%% References with bibTeX database:


\section*{References}

\bibliographystyle{elsarticle-harv}
\bibliography{references}


%% Authors are advised to submit their bibtex database files. They are
%% requested to list a bibtex style file in the manuscript if they do
%% not want to use model1-num-names.bst.

%% References without bibTeX database:

% \begin{thebibliography}{00}

%% \bibitem must have the following form:
%%   \bibitem{key}...
%%

% \bibitem{}

% \end{thebibliography}

\end{document}

%%
%% End of file `elsarticle-template-1-num.tex'.
