%% This is file `elsarticle-template-1-num.tex',
%%
%% Copyright 2009 Elsevier Ltd
%%
%% This file is part of the 'Elsarticle Bundle'.
%% ---------------------------------------------
%%
%% It may be distributed under the conditions of the LaTeX Project Public
%% License, either version 1.2 of this license or (at your option) any
%% later version.  The latest version of this license is in
%%    http://www.latex-project.org/lppl.txt
%% and version 1.2 or later is part of all distributions of LaTeX
%% version 1999/12/01 or later.
%%
%% The list of all files belonging to the 'Elsarticle Bundle' is
%% given in the file `manifest.txt'.
%%
%% Template article for Elsevier's document class `elsarticle'
%% with numbered style bibliographic references
%%
%% $Id: elsarticle-template-1-num.tex 149 2009-10-08 05:01:15Z rishi $
%% $URL: http://lenova.river-valley.com/svn/elsbst/trunk/elsarticle-template-1-num.tex $
%%

\documentclass[final,5p,times,twocolumn]{elsarticle}

%% Use the option review to obtain double line spacing
%% \documentclass[preprint,review,12pt]{elsarticle}

%% Use the options 1p,two column; 3p; 3p,twocolumn; 5p; or 5p,twocolumn
%% for a journal layout:
%% \documentclass[final,1p,times]{elsarticle}
%% \documentclass[final,1p,times,twocolumn]{elsarticle}
%% \documentclass[final,3p,times]{elsarticle}
%% \documentclass[final,3p,times,twocolumn]{elsarticle}
%% \documentclass[final,5p,times]{elsarticle}
%% \documentclass[final,5p,times,twocolumn]{elsarticle}

\usepackage{epigraph}
\usepackage{color}
\usepackage{multirow,booktabs,ctable,array}
\usepackage{lscape}
\usepackage{amsmath}
\usepackage{lineno}
%%\usepackage{ulem}
\usepackage{setspace}
\usepackage{listings}
\usepackage{float}
\usepackage{listings}
\usepackage{color,colortbl}
\usepackage{rccol}
\usepackage[colorlinks=true,linkcolor=blue]{hyperref}
\usepackage[table]{xcolor}

    \definecolor{listcomment}{rgb}{0.0,0.5,0.0}
    \definecolor{listkeyword}{rgb}{0.0,0.0,0.5}
    \definecolor{listnumbers}{gray}{0.65}
    \definecolor{listlightgray}{gray}{0.955}
    \definecolor{listwhite}{gray}{1.0}
    \definecolor{lightcyan}{rgb}{0.88,1,1}

\usepackage[draft]{changes}
\definechangesauthor[name={Hans Johnson}, color=green]{hj}
\definechangesauthor[name={Torsten Rolfing}, color=red]{tr}
\definechangesauthor[name={Nick Tustison}, color=orange]{nt}
\definechangesauthor[name={Brian Avants}, color=purple]{ba}
\definechangesauthor[name={Arno Klein}, color=blue]{ak}
\definechangesauthor[name={Satrajit Ghosh}, color=magenta]{sg}


\floatstyle{plain}
\newfloat{command}{thp}{lop}
\floatname{command}{Command}

%\usepackage[nomarkers,notablist]{endfloat}

%% if you use PostScript figures in your article
%% use the graphics package for simple commands
%% \usepackage{graphics}
%% or use the graphicx package for more complicated commands
%% \usepackage{graphicx}
%% or use the epsfig package if you prefer to use the old commands
%% \usepackage{epsfig}

%% The amssymb package provides various useful mathematical symbols
\usepackage{amssymb}
%% The amsthm package provides extended theorem environments
% \usepackage{amsthm}

 \usepackage{makecell}

%% The lineno packages adds line numbers. Start line numbering with
%% \begin{linenumbers}, end it with \end{linenumbers}. Or switch it on
%% for the whole article with \linenumbers after \end{frontmatter}.
%% \usepackage{lineno}

%% natbib.sty is loaded by default. However, natbib options can be
%% provided with \biboptions{...} command. Following options are
%% valid:

%%   round  -  round parentheses are used (default)
%%   square -  square brackets are used   [option]
%%   curly  -  curly braces are used      {option}
%%   angle  -  angle brackets are used    <option>
%%   semicolon  -  multiple citations separated by semi-colon
%%   colon  - same as semicolon, an earlier confusion
%%   comma  -  separated by comma
%%   numbers-  selects numerical citations
%%   super  -  numerical citations as superscripts
%%   sort   -  sorts multiple citations according to order in ref. list
%%   sort&compress   -  like sort, but also compresses numerical citations
%%   compress - compresses without sorting
%%
%% \biboptions{comma,round}

% \biboptions{}

\providecommand{\OO}[1]{\operatorname{O}\bigl(#1\bigr)}

\graphicspath{
             {./Figures/}
             }

\long\def\symbolfootnote[#1]#2{\begingroup%
\def\thefootnote{\fnsymbol{footnote}}\footnote[#1]{#2}\endgroup}

% \usepackage{Sweave}
\begin{document}
% \SweaveOpts{concordance=TRUE}
% \input{Rpaper-concordance}


\begin{frontmatter}

\title{Instrumentation bias in the use and evaluation of scientific
software\added[id=tr,remark={Just a suggestion}]{: Recommendations for reproducible
\replaced[id=nt]{practices in the computational sciences}{experiments}}}

\author[label1]{Nicholas J.~Tustison
  \fnref{label0}}
  \fntext[label0]{\scriptsize Corresponding author:  PO Box 801339, Charlottesville, VA 22908; T:  434-924-7730; email address:  ntustison@virginia.edu.
  }
\author[label3]{Hans Johnson}
\author[label4]{Torsten Rohlfing}
\author[label5]{Arno Klein}
\author[label6]{Satrajit S.~Ghosh}
\author[label7]{Luis Ibanez}
\author[label2]{Brian B.~Avants}
\address[label1]{Department of Radiology and Medical Imaging, University of Virginia, Charlottesville, VA, USA}
\address[label2]{Penn Image Computing and Science Laboratory, University of Pennsylvania,
                Philadelphia, PA, USA}
\address[label3]{Department of Psychiatry, University of Iowa, Iowa City, IA, USA}
\address[label4]{SRI International, Menlo Park, CA, USA}
\address[label5]{Department of Psychiatry and Behavioral Science, State University of New York, Stony Brook, NY, USA}
\address[label6]{McGovern Institute	for	Brain	Research, Massachusetts Institute of Technology, Cambridge, MA, USA}
\address[label7]{Kitware, Inc., Clifton Park, NY, USA}

%\maketitle

\linenumbers

\begin{abstract}
\replaced[id=tr,remark={Title, article, and journal are all broader than just ''image-based''!}]{Current}{Image-based} science relies heavily on software resources\added[id=tr]{.} 
\deleted[id=tr]{for population
and subject-specific multiple modality studies.  New imaging
technologies and application domains lead to continuous algorithmic
innovation.}  A standard part of developing and vetting new
approaches to data transformation and analysis involves comparison with other methods previously
established within the community.  Public availability, including
open source distribution, of established packages has significantly
facilitated these comparative evaluations, particularly when the packages
provide access to an open source distribution and are indexed in a resource like the 
Neuroimaging Informatics Tools and Resources Clearinghouse%
\footnote{
http://www.nitrc.org
}.  While we \replaced[id=tr]{welcome and support}{applaud} all efforts to evaluate such
technological developments (which are  critical to establishing the methods 
as viable translational tools) the lack of guiding principles for comparative
evaluations can result in critical susceptibility to {\em instrumentation bias}.
In this
case, the bias occurs most strongly when authors of a new algorithm
also conduct the algorithm's evaluation, frequently by
running their own experiments against a set of ``competing methods.''
The problems with this are obvious and nefarious:  in brief, authors want to show
that their method is ``the best'' and allocate resources accordingly.  
In this paper we propose a set of guidelines that will help authors and 
reviewers minimize the presence of this type of bias as it applies to
computational methods\added[id=tr]{, specifically} in\added[id=tr]{, but not
limited to,} the imaging sciences.
\end{abstract}
\begin{keyword}
open science \sep open data \sep comparison \sep reproducibility  \sep software \sep instrument bias
%% keywords here, in the form: keyword \sep keyword
\end{keyword}

\end{frontmatter}
%
%
\newpage

%% MSC codes here, in the form: \MSC code \sep code
%% or \MSC[2008] code \sep code (2000 is the default)

%%
%% Start line numbering here if you want
%%
% \linenumbers

%% The Appendices part is started with the command \appendix;
%% appendix sections are then done as normal sections
%% \appendix

%% \section{}
%% \label{}

%% References
%%
%% Following citation commands can be used in the body text:
%% Usage of \cite is as follows:
%%   \citep{key}          ==>>  [#]
%%   \cite[chap. 2]{key} ==>>  [#, chap. 2]
%%   \citet{key}         ==>>  Author [#]

%% main text


\section{Introduction}

\epigraph{By honest I don't mean that you only tell what's true. But you make clear the
entire situation. You make clear all the information that is required for somebody else
who is intelligent to make up their mind.}{Richard Feynman}


The neuroscience community significantly benefits from the proliferation
of imaging-related analysis software packages.
Established packages such as SPM \cite{Ashburner2012},
the FMRIB Software Library (FSL) \cite{Jenkinson2012}, Freesurfer
\cite{fischl2012}, Slicer \cite{fedorov2012} and
the AFNI toolkit \cite{cox2012} aid neuroimaging researchers
around the world in performing complex analyses as part of
ongoing neuroscience research.
In conjunction with distributing robust software tools, neuroimaging 
packages also continue to incorporate
algorithmic innovation for improvement
in analysis tools.

As fellow scientists who actively participate in neuroscience
research through our contributions to the Insight
Toolkit
(e.g. \cite{johnson2007,tustison2009a,tustison2012a})
and other packages such as the Advanced Normalization Tools (ANTs),%
\footnote{
http://www.picsl.upenn.edu/ANTs
} \cite{avants2011,avants2010}
we notice an increasing number of publications that intend a
fair comparison of algorithms which, in principle, is
a good thing.  Our concern is the lack of detail with 
which these comparisons are
often presented and the corresponding possibility of {\it instrumentation
bias} \cite{sackett1979} (considering software  as a type of
instrument requiring proper ``calibration'' for accurate measurements).
Based on our experience (including our own mistakes),
we propose a preliminary set of guidelines that seek to minimize such bias,
\added[id=tr]{with the} understanding that
the discussion will require a more comprehensive response from the larger neuroscience community.
We intend this commentary to raise awareness in both
authors and reviewers to issues that arise when comparing quantitative
algorithms\replaced[id=tr]{. Although herein we focus specifically on image
registration, }{ for image registration.  While image registration is our
focus,} these recommendations are relevant for \deleted[id=tr]{several} other
application areas in biologically-focused computational 
image analysis\replaced[id=nt]{, and for reproducible computational science in general}{\added[id=tr]{, and for reproducible computational experiments in general}}.  This comment complements recent papers that highlight
statistical bias \cite{kriegeskorte2009,vul2012}, bias induced by
registration metrics \cite{tustison2012}, and registration strategy \cite{yushkevich2010} and guideline papers for
software development \cite{prlic2012}.
%
% (not unlike other
% recent articles detailing additional potential sources of methodological
% bias).

\section{Guidelines}

A comparative analysis paper's longevity and impact on future scientific
explorations is directly related to the completeness of the evaluation.  
A complete evaluation requires preparatory effort (before any
experiment is performed) and effort to publish its details and results.
Here, we suggest general guidelines for both of these steps most of which
derive from basic scientific principles of clarity and reproducibility.

\subsection{Designing the evaluation study}
\subsubsection{Rigorously define the algorithm's scope}
% The only algorithm that is capable of outperforming all others on all
% problems is exhaustive search, assuming a perfect objective function.
% As this is not feasible in practice, many algorithms employ
% prior knowledge such as regularization or other heuristics that
% restrict the search space to the relevant parameters for a narrowly 
% specified problem.  

The very idea that one (e.g. registration) algorithm 
could perform better than all other algorithms on
all types of data is fundamentally flawed.  Indeed, the ``No Free Lunch
Theorems'' provide\added[id=nt]{s} bounds on solution quality that show otherwise
i.e. \added[id=nt]{it specifically demonstrates} that ``improvement 
of performance in problem-solving
hinges on using prior information to match procedures to
problems''\cite{Wolpert1997}. Therefore, the first thing that authors of new algorithms should do is
identify how their methods differ with respect to other
\deleted[id=tr,remark={This shouldn't make a difference for the discussion of scope.}]{freely}
available techniques in terms of the use of prior knowledge.
Furthermore, is it possible to incorporate similar prior knowledge
into existing methods\replaced[id=tr,remark={Not well worded... ``incorporation'' doesn't happen
just by asking.}]{? }{ simply by asking the authors of these algorithms
how this might be achieved?}

\subsubsection{Demand that the algorithm developers provide default parameters for the 
context being investigated}
\added[id=tr]{REMARK - this is a recommendation more geared towards
rewviewers, wheras most others a geared towards authors. I think we should
clearly separate the two, maybe different subsections?}

Expert knowledge of a specific program and/or algorithm is most
likely found with the original developers who would be in a position to
provide optimal parameterization.  To help this procedure be fruitful,
one should be willing to share example datasets.  Sharing data with
algorithm developers allows the developers to facilitate
comparisons by providing relevant parameter files and sample scripts
that detail command line calls.  For example, the developers of
the image registration program elastix \cite{klein2010} provide an
assortment of parameter files on the wiki page%
\footnote{
  http://elastix.bigr.nl/wiki/index.php/Parameter\_file\_database
}
listed in tabular format complete with short
description (including applied modality and object of interest) and
any publications which used that specific parameter file.  

\subsubsection{Do not implement your own version of the algorithm particularly
if one is available from the original authors.
If you must re-implement, consider making your implementation available}

Much is left unstated in published manuscripts where 
novel algorithmic concepts are presented.  Ideally, the authors 
provide an instantiation of the code to accompany the 
manuscript.  As observed in \cite{kovacevic2006}, however,
this is often not the case (even in terms of pseudocode).
As a result, comparative evaluations are sometimes carried out using
code developed not by the original authors but by the
group doing the comparison.   For
example, in \cite{clarkson2011}, the authors compared three algorithms
for estimating cortical thickness.  Two of the algorithms were
coded by the authors of the study while the third was used ``off the shelf'' 
having been extensively
tuned and well-documented by the original authors.
Thus, a natural question to ask is whether the performance difference
is due to the algorithm itself, implementation quality, and/or the parameter 
tuning.  None of these are addressed by \cite{clarkson2011} which may decrease the publication's usefulness.

\subsubsection{Co-authors should verify findings}

\added[id=tr]{REMARK - this section would fit better under ``Publishing the evaluation''.}

Although different journals have varying guidelines for determining
co-authorship, there is at least an implied sense of responsibility for
an article's contents assumed by each of the co-authors.  Strategies
taken by journal editorial boards are used to reduce undeserving
authorship attribution such as requiring the listing of the specific
contributions of each co-author.  Additional proposals have included
signed statements of responsibility for the contents of \replaced[id=tr]{an}{the} article
\cite{nature2007}.  We suggest that at least one co-author
independently verify a subset of the results by running the
\replaced[id=tr]{data processing and analysis}{processing}
on their own \replaced[id=nt]{computational platform}{computer}.   
The point of this exercise is to verify not only
reproducibility but also that the process can be explained in
sufficient detail.

\subsubsection{Comparisons should be performed on publicly available data}
For reasons of reproducibility and transparency, evaluations should be performed
using publicly available data sets.  Given the rather large number
of such institutional efforts including
IXI,%
\footnote{
http://www.brain-development.org
}
NKI,%
\footnote{
http://fcon\_1000.projects.nitrc.org/indi/pro/nki.html
}
OASIS,%
\footnote{
http://www.oasis-brains.org
}
Kirby,%
\footnote{
http://mri.kennedykrieger.org/databases.html
},
LONI,%
\footnote{
http://www.loni.ucla.edu/Research/Databases/
}, and
others, evaluations should include (if not be exhausted by)
comparisons using such data.  While evaluation on \replaced[id=nt]{private}
{specific abnormal} cohorts
might exclude such possibilities, such specific evaluations should
be extensively motivated in the introduction and/or discussion.
For example, if a particular algorithm with general application is
found to perform better
on a private cohort of Parkinson's disease subject data, reasons
for performance disparity should be offered and supplemented with
analysis on public data.


\subsection{Publishing the evaluation}

\subsubsection{Include parameters}
In \cite{klein2009},
14 nonlinear registration algorithms were compared using
four publicly available, labeled brain MRI data sets.
As part of the study, the respective algorithms' authors tuned
the parameters to ensure good
performance which were then distributed 
on Prof. Klein's website.%
\footnote{
http://mindboggle.info/papers/evaluation\_NeuroImage2009.php
}
In contrast, not specifying parameters
leaves one susceptible to criticisms of confirmation and/or
instrumentation bias \cite{sackett1979}.  For example, in
a recent paper \cite{haegelen2013} (similar issues can
be found in \cite{luo2013,wu2013}), the authors compared
their ANIMAL registration algorithm with SyN \cite{avants2011} and determined
that ``\replaced[id=tr,remark={It is generally acceptable to change
capitalization of a quote's first words without markup to accomodate the
grammatical context.}]{r}{[r]}egistration with ANIMAL was better than with SyN for
the left thalamus'' in a cohort of Parkinson's disease patients.
One of the stated reasons for using ANIMAL was ``because of
[the authors']
extensive experience with this technique.'' However, for SyN,
the authors provide no equivalent assurance---\replaced[id=tr,remark={This part
is really not related to the ``include parameters'' recommendation.}]{no parameters were}{contact
was made with the primary developers of ANTs (SyN) nor
were any parameters} listed for independent verification
by reviewers or readers of the article.


\subsubsection{Provide details as to the source of the algorithm}
Origin 
should be provided for any code or package used during the
evaluation.  For example, N4 \cite{tustison2010} is a well-known
bias correction algorithm first made available as a tech report
 \cite{tustison2009a}.  However, since
its inclusion in the Insight Toolkit, different instantiations 
have been made available. N4 is also available in ANTs (the only 
version directly maintained by the original authors), as a module in Slicer,%
\footnote{
http://www.slicer.org/slicerWiki/index.php/Documentation/4.2/Modules/N4ITKBiasFieldCorrection
}
a wrapper of the Slicer module in nipype,%
\footnote{
http://www.mit.edu/~satra/nipype-nightly/interfaces/generated/nipype.interfaces.slicer.filtering.n4itkbiasfieldcorrection.html
}
a module in \verb#c3d#,%
\footnote{
http://www.itksnap.org8/pmwiki/pmwiki.php?n=Convert3D.Documentation
}
and as a plugin in the BRAINS suite.%
\footnote{
http://www.nitrc.org/plugins/mwiki/index.php/brains:N4ITK
}
While each version is dependent on the original source
code, there could exist subtle variations which can affect performance.
As \replaced[id=tr]{one specific example}{a particular case}, the \verb#c3d#
implementation hard-codes
\deleted[id=tr]{the} certain parameter values with no access to modify them by
the user\replaced[id=tr,remark={Or maybe it performs better for it. Anyway,
you are making your point without needing to predict whether results with fixed
parameters are always better or worse.}]{.}{, 
potentially leading to suboptimal performance.}

\subsubsection{Provide computational platform details of the evaluation}
A recent article \cite{gronenschild2012} pointed out significant
differences in FreeSurfer output that varied with  release
version and with operating systems.  While the former is to be
expected given upgrades and bug fixes which occur between releases,
the latter underscores both the need for consistency in study processing
as well as the reporting of computational details for reproducibility.

\subsubsection{Pre- and post-processing steps need to be supplied}
In addition to disclosure of all parameters associated with the
methods to be compared, all processing steps from the raw
to the final processed images in the workflow
need to be specified.    Justification for any deviation
of steps between algorithms needs to be provided.  Additionally,
failures during processing should be mentioned and whether
or not these data were excluded from subsequent analysis.


\subsubsection{Resulting \replaced[id=tr]{data}{images} should be
\replaced[id=tr]{made available online}{publicly posted 
and/or online public resources should be used}
}

The current publishing paradigm limits the quantity of
results that can be posted.  There are only so many pages
allowed for a particular publication and displaying every
slice of every processed image, for example, is not feasible.
This results in possible selection bias where results provided
in the manuscript are selected by the authors for demonstrating
the effect postulated at the onset of the study.  Thus, differences
in performance assessment tend to be exaggerated based strictly
on visual representations in the paper.
\added[id=tr]{Publication simply in print (or as figures in a PDF file) and
its limitations in terms of dynamic range or spatial resolution also
severely limits the ability of reviewers and readers to perform more
sophisticated evaluation beyond simple visual inspection.}

Alternatively (or additionally), online resources such as the
the LONI Segmentation Validation Engine \cite{shattuck2009}%
\footnote{
http://sve.loni.ucla.edu
}
can be used to evaluate individual algorithms
for brain segmentation on publicly available
data sets and compare with previously posted results.  A top ranking
outcome provides significant external validation for publishing newly
proposed methodologies (e.g. \cite{eskildsen2012}).

%figshare

\subsubsection{\replaced[id=tr]{Put comparisons and observed performance differences into context}{Properly contextualize comparisons and motivate performance differences}}
In addition to algorithmic and study specifics, it is important to
discuss potential limitations concerning qualitative and/or quantitative
assessment metrics.  In \cite{rohlfing2012}, T. Rohlfing pointed out
deficiencies in using standard overlap measures and image similarity
metrics in quantifying performance\added[id=tr]{ of image registration methods}.  Other issues, such as biological
plausibility of the resulting transforms, need to also be considered.
Also important for inclusion is discussion on the possible reasons for performance
disparity.  If one algorithm outperforms another, reporting of those findings
would be much more significant if the authors discussed possible reasons
for relative  performance levels.  

\subsection{Relevance of the current publishing paradigm}
\added[id=tr]{REMARK -- sorry, but is this section really at all relevant to
the topic of the article?}
There is a very active discussion within the broad scientific
community regarding current publishing practices.  Issues 
such as open access to scientific findings and exorbitant 
subscription costs cause many scientists, and even institutions, to rethink
their approaches to reporting research.  In addition to
such issues,
we point out that these traditional venues are often 
suboptimal for publishing evaluations given the dynamic nature
of the software development process.    In
addition to considering public availability, we also encourage
authors to consider more dynamic venues for posting relevant material
and updates.  Online utilities such as \href{http://www.figshare.com}{figshare}, \href{http://www.sourceforge.net}{sourceforge} and \href{http://www.github.com}{github} not only facilitate
collaboration and code distribution but often support wikis
and discussion forums which provide an auxiliary resource to
original publication(s).  Participating in open
evaluations conducted by third-party organizers
(e.g. \cite{klein2009,murphy2011}) is often an effective (and occasionally high profile) approach for obtaining unbiased algorithm comparison.

\section{Conclusion}
Considering that computational sciences permeate neuroimaging research, certain safeguards should be in place to prevent
(or at least minimize) potential biases and errors that can
unknowingly effect study outcomes.  There is no vetting agency for
ensuring that analysis programs used for research are reasonably error-free.
In addition, these software packages are simply ``black
boxes'' to many researchers who are not formally trained to debug
code\added[id=tr,remark={I like to call this ``The SPM Effect'' - even a room
full of monkeys with an MR scanner and a Matlab license will eventually find a
``significant'' fMRI or VBM finding.}]{, and who, in most cases, have only a
very superficial understandings of the algorithms that they apply}.  And even
to those of us who are, \replaced[id=tr]{understanding someone else's code, perhaps
implemented in an unfamiliar programming language and poor coding style,}{debugging other people's code} 
is oftentimes very difficult.  To this end, algorithmic comparisons are
a very good way of evaluating general performance.  We hope that the
guidelines proposed in this editorial help the community
in future comparative assessments and avoid errors in scientific computing that
may \replaced[id=tr]{otherwise lead to}{lead, ultimately, to} publication of invalid results \cite{merali2010}.

%% References with bibTeX database:


\section*{References}

\bibliographystyle{elsarticle-harv}
\bibliography{references}


%% Authors are advised to submit their bibtex database files. They are
%% requested to list a bibtex style file in the manuscript if they do
%% not want to use model1-num-names.bst.

%% References without bibTeX database:

% \begin{thebibliography}{00}

%% \bibitem must have the following form:
%%   \bibitem{key}...
%%

% \bibitem{}

% \end{thebibliography}

\listofchanges

\end{document}

%%
%% End of file `elsarticle-template-1-num.tex'.
